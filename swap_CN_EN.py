import copy as cp
import os
import csv
import json
import re
import tempfile
import datetime
import shutil

from typing import Dict, List, Any, Union
from pathlib import Path

# -------------------------- å…¨å±€ä¸´æ—¶æ–‡ä»¶/é‡å‘½åä»»åŠ¡ç®¡ç† --------------------------
# å­˜å‚¨æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶è·¯å¾„æ˜ å°„åˆ°åŸæ–‡ä»¶ï¼Œç”¨äºæ‰¹é‡æ›¿æ¢/æ¸…ç†
TEMP_FILES: Dict[str, str] = {}
# å­˜å‚¨é‡å‘½åä»»åŠ¡ï¼Œæ‰¹é‡æ‰§è¡Œ
RENAME_TASKS: List[tuple] = []
# å­˜å‚¨éœ€è¦æ¸…ç†çš„_EN/_CNæ–‡ä»¶è·¯å¾„ï¼ˆç»Ÿä¸€æ‰¹é‡æ¸…ç†ï¼‰
TO_CLEAN_SUFFIX_FILES: List[tuple] = []

# å…¨å±€å½“å‰è¯­è¨€è®¾ç½®ï¼ˆç”± data/config/settings.json çš„ aEP_UseEnString å†³å®šï¼‰
USE_EN_SETTING_OLD: Union[bool, None] = None
USE_EN_SETTING_NEW: Union[bool, None] = None

# å¤‡ä»½ç›®å½•ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_BACKUP_DIR: Union[str, None] = None

# -------------------------- ä¸´æ—¶æ–‡ä»¶æ“ä½œå‡½æ•° --------------------------
def create_temp_file(content: str, original_file_path: str) -> str:
    """
    åˆ›å»ºä¸´æ—¶æ–‡ä»¶ï¼ˆä¸åŸæ–‡ä»¶åŒç›®å½•ï¼‰ï¼Œè¿”å›ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    :param content: è¦å†™å…¥ä¸´æ—¶æ–‡ä»¶çš„å†…å®¹
    :param original_file_path: åŸæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºç¡®å®šä¸´æ—¶æ–‡ä»¶ä½ç½®ï¼‰
    :return: ä¸´æ—¶æ–‡ä»¶ç»å¯¹è·¯å¾„
    """
    original_path = Path(original_file_path)
    # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶ï¼ˆåç¼€.tmpï¼Œä¸åŸæ–‡ä»¶åŒç›®å½•ï¼‰
    temp_fd, temp_path = tempfile.mkstemp(
        suffix='.tmp',
        prefix=original_path.stem + '_',
        dir=str(original_path.parent)
    )
    # å†™å…¥å†…å®¹å¹¶å…³é—­æ–‡ä»¶å¥æŸ„ï¼ˆUTF-8ç¼–ç ï¼Œä¿ç•™ä¸­æ–‡ï¼‰
    with os.fdopen(temp_fd, 'w', encoding='utf-8') as f:
        f.write(content)
    # è®°å½•ä¸´æ—¶æ–‡ä»¶è·¯å¾„ -> åŸæ–‡ä»¶è·¯å¾„ æ˜ å°„ï¼ˆç”¨äºåŸå­æ›¿æ¢æ—¶æ¢å¤åŸå/æ‰©å±•åï¼‰
    TEMP_FILES[temp_path] = str(original_path)
    return temp_path

def write_to_temp_csv(rows: List[Dict[str, str]], original_file_path: str) -> str:
    """
    å°†CSVæ•°æ®å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆä¿ç•™ä¸­æ–‡é€—å·ï¼Œä»…å¤„ç†åŠè§’é€—å·è½¬ä¹‰ï¼‰
    :param rows: CSVè¡Œæ•°æ®
    :param original_file_path: åŸæ–‡ä»¶è·¯å¾„
    :return: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    """
    if not rows:
        raise ValueError("æ— æ•°æ®å¯å†™å…¥ä¸´æ—¶CSVæ–‡ä»¶")
    # æ„å»ºCSVå†…å®¹
    fieldnames = list(rows[0].keys())
    csv_content_lines: List[str] = []
    # è¡¨å¤´ï¼ˆåŠè§’é€—å·åˆ†éš”ï¼‰
    csv_content_lines.append(','.join(fieldnames))

    for row in rows:
        escaped_row = []
        # ensure we iterate in header order so columns align
        for k in fieldnames:
            v = row.get(k, '')
            val_str = '' if v is None else str(v)
            # ä»…å¤„ç†CSVæ ‡å‡†è½¬ä¹‰ï¼šå«åŠè§’é€—å·/åŒå¼•å·/æ¢è¡Œç¬¦çš„å­—æ®µéœ€ç”¨åŒå¼•å·åŒ…è£¹
            if (',' in val_str) or ('"' in val_str) or ('\n' in val_str):
                val_str = val_str.replace('"', '""')  # åŒå¼•å·è½¬ä¹‰ä¸ºä¸¤ä¸ª
                val_str = f'"{val_str}"'  # åŒ…è£¹åŒå¼•å·
            # ä¸­æ–‡é€—å·ï¼ˆï¼Œï¼‰ä¸åšä»»ä½•å¤„ç†ï¼Œä¿ç•™ä¸ºå­—ç¬¦ä¸²å†…å®¹
            escaped_row.append(val_str)
        csv_content_lines.append(','.join(escaped_row))  # åŠè§’é€—å·åˆ†éš”å­—æ®µ

    csv_content = '\n'.join(csv_content_lines)
    # å†™å…¥ä¸´æ—¶æ–‡ä»¶
    return create_temp_file(csv_content, original_file_path)

def write_to_temp_json(data: Union[Dict[str, Any], List[Any]], original_file_path: str) -> str:
    """
    å°†JSONæ•°æ®å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆä¿ç•™ä¸­æ–‡é€—å·ï¼ŒUTF-8ç¼–ç ï¼‰
    :param data: JSONå­—å…¸/åˆ—è¡¨æ•°æ®
    :param original_file_path: åŸæ–‡ä»¶è·¯å¾„
    :return: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    """
    # å¢å¼ºç©ºæ•°æ®æ ¡éªŒï¼Œæ·»åŠ æ—¥å¿—æç¤ºå…·ä½“æ–‡ä»¶
    if data is None:
        raise ValueError(f"æ— æ•°æ®å¯å†™å…¥ä¸´æ—¶JSONæ–‡ä»¶ï¼ˆæ–‡ä»¶è·¯å¾„ï¼š{original_file_path}ï¼‰")
    # æ ¼å¼åŒ–JSONå†…å®¹ï¼ˆensure_ascii=Falseä¿ç•™ä¸­æ–‡ï¼Œindent=2ä¿æŒç¼©è¿›ï¼‰
    json_content = json.dumps(data, ensure_ascii=False, indent=2)
    # å†™å…¥ä¸´æ—¶æ–‡ä»¶
    return create_temp_file(json_content, original_file_path)

def batch_replace_original_files() -> None:
    """æ‰¹é‡å°†ä¸´æ—¶æ–‡ä»¶æ›¿æ¢ä¸ºåŸæ–‡ä»¶ï¼ˆåŸå­æ“ä½œï¼Œè·¨å¹³å°å…¼å®¹ï¼‰"""
    # iterate over a snapshot because we'll modify TEMP_FILES inside loop
    for temp_path, original_path in list(TEMP_FILES.items()):
        try:
            if os.path.exists(temp_path) and os.path.exists(original_path):
                # ä½¿ç”¨os.replaceä¿è¯åŸå­æ›¿æ¢ï¼ˆè·¨å¹³å°ï¼Œé¿å…æ–‡ä»¶æŸåï¼‰
                os.replace(temp_path, original_path)
                print(f"âœ… åŸå­æ›¿æ¢å®Œæˆï¼š{original_path} â† {temp_path}")
            elif os.path.exists(temp_path) and not os.path.exists(original_path):
                # åŸæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥é‡å‘½åä¸´æ—¶æ–‡ä»¶ä¸ºåŸæ–‡ä»¶
                os.rename(temp_path, original_path)
                print(f"âœ… é‡å‘½åä¸´æ—¶æ–‡ä»¶ä¸ºåŸæ–‡ä»¶ï¼š{temp_path} â†’ {original_path}")
            else:
                # temp file missing or both missing
                print(f"âš ï¸ ä¸´æ—¶æ–‡ä»¶æˆ–åŸæ–‡ä»¶ä¸å­˜åœ¨ï¼š{temp_path} / {original_path}")
        finally:
            # æ— è®ºæˆåŠŸä¸å¦ï¼Œç§»é™¤æ˜ å°„ä»¥é¿å…é‡å¤å¤„ç†
            TEMP_FILES.pop(temp_path, None)

def clean_temp_files() -> None:
    """æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…æ®‹ç•™"""
    for temp_path in list(TEMP_FILES.keys()):
        if os.path.exists(temp_path):
            try:
                os.remove(temp_path)
                print(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼š{temp_path}")
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {temp_path}ï¼š{e}")
        # ç§»é™¤æ˜ å°„
        TEMP_FILES.pop(temp_path, None)
    # æ¸…ç©ºä¸´æ—¶æ–‡ä»¶æ˜ å°„è¡¨
    TEMP_FILES.clear()

# -------------------------- é€šç”¨å·¥å…·å‡½æ•° --------------------------
def clean_json_content(raw_content: str) -> str:
    """
    æ¸…ç†JSONå†…å®¹ï¼ˆä¿ç•™ä¸­æ–‡é€—å·ï¼Œä»…å¤„ç†è¯­æ³•çº§é—®é¢˜ï¼‰ï¼š
    - ç§»é™¤ä¸å¯è§æ§åˆ¶å­—ç¬¦
    - ç»Ÿä¸€æ¢è¡Œ
    - ç§»é™¤ BOM
    - ç§»é™¤è¡Œå†…/è¡Œå°¾çš„ # æ³¨é‡Šï¼ˆä½†ä¿ç•™å­—ç¬¦ä¸²å†…çš„ #ï¼‰
    - ç§»é™¤æœ«å°¾å¤šä½™çš„åŠè§’é€—å·
    """
    # æ­¥éª¤1ï¼šç§»é™¤ä¸å¯è§æ§åˆ¶å­—ç¬¦ï¼ˆé¿å…å¹²æ‰°è§£æï¼Œä¸ç¢°ä¸­æ–‡é€—å·ï¼‰
    control_chars = [
        '\u200b', '\u200c', '\u200d',  # é›¶å®½ç©ºæ ¼/è¿æ¥ç¬¦
        '\x00', '\x01', '\x02', '\x03', '\x04', '\x05',  # ç©ºå­—ç¬¦/æ§åˆ¶å­—ç¬¦
        '\v', '\f', '\x1c', '\x1d', '\x1e', '\x1f'  # å‚ç›´åˆ¶è¡¨ç¬¦/æ¢é¡µç¬¦
    ]
    for char in control_chars:
        raw_content = raw_content.replace(char, '')

    # æ­¥éª¤2ï¼šç»Ÿä¸€æ¢è¡Œç¬¦ä¸º\n
    raw_content = raw_content.replace('\r\n', '\n').replace('\r', '\n')

    # æ­¥éª¤3ï¼šç§»é™¤UTF-8 BOMå¤´
    if raw_content.startswith('\ufeff'):
        raw_content = raw_content.lstrip('\ufeff')

    # æ­¥éª¤4ï¼šæŒ‰å­—ç¬¦éå†ç§»é™¤ # æ³¨é‡Šï¼Œä½†ä¿ç•™å­—ç¬¦ä¸²å†…çš„å†…å®¹
    out_chars: List[str] = []
    in_string = False
    string_quote = ''
    escape = False
    i = 0
    length = len(raw_content)
    while i < length:
        ch = raw_content[i]
        if escape:
            # previous was backslash, so this char is escaped inside string
            out_chars.append(ch)
            escape = False
            i += 1
            continue
        if in_string:
            if ch == '\\':
                # start escape sequence
                out_chars.append(ch)
                escape = True
                i += 1
                continue
            out_chars.append(ch)
            if ch == string_quote:
                in_string = False
                string_quote = ''
            i += 1
            continue
        # not in string
        if ch == '"' or ch == "'":
            in_string = True
            string_quote = ch
            out_chars.append(ch)
            i += 1
            continue
        if ch == '#':
            # skip until newline (remove the comment). keep the newline if present
            # advance i to next newline or EOF
            while i < length and raw_content[i] != '\n':
                i += 1
            # if newline exists, append it to preserve line breaks
            if i < length and raw_content[i] == '\n':
                out_chars.append('\n')
                i += 1
            continue
        # normal char outside string
        out_chars.append(ch)
        i += 1

    clean_content = ''.join(out_chars)

    # æ­¥éª¤5ï¼šç§»é™¤æœ«å°¾å¤šä½™åŠè§’é€—å·ï¼ˆåœ¨ ] æˆ– } ä¹‹å‰çš„é€—å·ï¼‰
    # ä½¿ç”¨æ­£åˆ™å»é™¤é€—å·åé¢åªè·Ÿç©ºç™½å†è·Ÿ ] æˆ– }
    clean_content = re.sub(r',\s*(?=[\]}])', '', clean_content)

    # æœ€åå»é™¤å¤šä½™ç©ºè¡Œå¹¶å³ä¾§ç©ºç™½
    lines = [ln.rstrip() for ln in clean_content.split('\n') if ln.strip() != '']
    return '\n'.join(lines)

def read_json_safely(file_path: str) -> Union[Dict[str, Any], List[Any]]:
    """
    å®‰å…¨è¯»å–JSONæ–‡ä»¶ï¼š
    1. ä¿ç•™å­—ç¬¦ä¸²å†…çš„ä¸­æ–‡é€—å·ï¼ˆï¼Œï¼‰
    2. è‡ªåŠ¨å¤„ç†UTF-8 BOMå¤´
    3. æç¤ºè¯­æ³•ä½ç½®çš„ä¸­æ–‡é€—å·é”™è¯¯
    4. æ–°å¢ï¼šç©ºæ•°æ®æ£€æµ‹å¹¶æç¤º
    """
    try:
        # ç”¨utf-8-sigç¼–ç è¯»å–ï¼Œè‡ªåŠ¨è¯†åˆ«å¹¶ç§»é™¤BOMå¤´
        with open(file_path, 'r', encoding='utf-8-sig') as f:
            raw_content = f.read()
        raw_content = raw_content.strip()

        # æ£€æµ‹ç©ºæ–‡ä»¶
        if not raw_content:
            raise ValueError(f"JSONæ–‡ä»¶å†…å®¹ä¸ºç©ºï¼š{file_path}")

        # æ¸…ç†æ³¨é‡Š/å¤šä½™åŠè§’é€—å·ï¼Œä¿ç•™ä¸­æ–‡é€—å·
        clean_content = clean_json_content(raw_content)

        # å†æ¬¡æ£€æµ‹æ¸…ç†åçš„æ•°æ®æ˜¯å¦ä¸ºç©º
        if not clean_content:
            raise ValueError(f"JSONæ–‡ä»¶æ¸…ç†åæ— æœ‰æ•ˆå†…å®¹ï¼š{file_path}")

        # è§£æJSON
        data = json.loads(clean_content)

        # æ£€æµ‹è§£æåçš„æ•°æ®æ˜¯å¦ä¸ºç©º
        if data is None or data == {} or data == []:
            raise ValueError(f"JSONæ–‡ä»¶è§£æåä¸ºç©ºï¼ˆç©ºå­—å…¸/åˆ—è¡¨ï¼‰ï¼š{file_path}")

        print(f"âœ… æˆåŠŸè¯»å–JSONæ–‡ä»¶ï¼ˆæœ‰æ•ˆæ•°æ®ï¼‰ï¼š{file_path}")
        return data

    except FileNotFoundError:
        raise
    except json.JSONDecodeError as e:
        error_msg = f"\nâŒ JSONè§£æé”™è¯¯ï¼ˆæ–‡ä»¶ï¼š{file_path}ï¼‰ï¼š{e}\n"
        error_msg += "âš ï¸ å¯èƒ½åŸå› ï¼š\n"
        error_msg += "  1. ä¸­æ–‡é€—å·ï¼ˆï¼Œï¼‰å‡ºç°åœ¨JSONè¯­æ³•ä½ç½®ï¼ˆå¦‚åˆ†éš”ç¬¦ï¼‰ï¼Œè¯·æ”¹ä¸ºåŠè§’é€—å·ï¼ˆ,ï¼‰ï¼›\n"
        error_msg += "  2. å­—ç¬¦ä¸²å†…çš„ä¸­æ–‡é€—å·æ— éœ€ä¿®æ”¹ï¼ˆå¦‚\"desc\": \"æµ‹è¯•ï¼Œå†…å®¹\"æ˜¯åˆæ³•çš„ï¼‰ã€‚\n"
        print(error_msg)
        # æ‰“å°é”™è¯¯ä½ç½®é™„è¿‘å†…å®¹ï¼Œä¾¿äºå®šä½é—®é¢˜
        error_range = clean_content[max(0, e.pos-10):e.pos+10] if 'clean_content' in locals() else "æ— æœ‰æ•ˆå†…å®¹"
        print(f"ğŸ“Œ é”™è¯¯ä½ç½®é™„è¿‘å†…å®¹ï¼š{repr(error_range)}")
        raise
    except Exception as e:
        print(f"\nâŒ è¯»å–JSONæ–‡ä»¶å¤±è´¥ï¼ˆæ–‡ä»¶ï¼š{file_path}ï¼‰ï¼š{e}")
        raise

def get_abs_file_path(relative_path: str) -> str:
    """è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„ï¼Œè§£å†³ç›¸å¯¹è·¯å¾„é—®é¢˜"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    return os.path.join(script_dir, relative_path)

def get_current_aep_setting() -> Union[bool, None]:
    """è¯»å– data/config/settings.json ä¸­çš„ aEP_UseEnStringï¼ˆåªè¯»ï¼‰ã€‚è¿”å› True/False æˆ– Noneï¼ˆæ— æ³•è¯»å–ï¼‰"""
    try:
        settings_path = get_abs_file_path('data/config/settings.json')
        settings = read_json_safely(settings_path)
        if isinstance(settings, dict) and 'aEP_UseEnString' in settings:
            return bool(settings['aEP_UseEnString'])
        return None
    except Exception:
        return None

def get_backup_dir() -> str:
    """è¿”å›æœ¬æ¬¡è¿è¡Œçš„å¤‡ä»½ç›®å½•è·¯å¾„ï¼ˆåœ¨é¡¹ç›®ä¸‹çš„ swapped_backups/<timestamp>/ï¼‰ï¼Œå¹¶ç¡®ä¿ç›®å½•å­˜åœ¨"""
    global _BACKUP_DIR
    if _BACKUP_DIR:
        return _BACKUP_DIR
    repo_root = os.path.dirname(os.path.abspath(__file__))
    ts = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    _BACKUP_DIR = os.path.join(repo_root, 'swapped_backups', ts)
    os.makedirs(_BACKUP_DIR, exist_ok=True)
    return _BACKUP_DIR

# ========== é€šç”¨åç¼€æ–‡ä»¶æ¸…ç†å‡½æ•°ï¼ˆä»…æ”¶é›†å¾…æ¸…ç†æ–‡ä»¶ï¼Œä¸ç«‹å³æ‰§è¡Œï¼‰ ==========
def collect_clean_task(base_dir: str, base_name: str, ext: str, use_en_setting: bool) -> None:
    """
    æ”¶é›†éœ€è¦æ¸…ç†çš„_EN/_CNæ–‡ä»¶ä»»åŠ¡ï¼ˆä¸ç«‹å³æ¸…ç†ï¼Œç»Ÿä¸€æ‰¹é‡æ‰§è¡Œï¼‰
    :param base_dir: æ–‡ä»¶æ‰€åœ¨ç›®å½•
    :param base_name: æ–‡ä»¶åŸºç¡€åï¼ˆä¸å«åç¼€å’Œ_EN/_CNï¼Œå¦‚ "submarkets"ï¼‰
    :param ext: æ–‡ä»¶æ‰©å±•åï¼ˆå¸¦ç‚¹ï¼Œå¦‚ .csvã€.jsonã€.factionï¼‰
    :param use_en_setting: å…¨å±€çš„USE_EN_SETTING_NEWå€¼ï¼ˆTrue=ä½¿ç”¨è‹±æ–‡ï¼Œä¿ç•™_CNï¼›False=ä½¿ç”¨ä¸­æ–‡ï¼Œä¿ç•™_ENï¼‰
    """
    # æ„å»ºEN/CNæ–‡ä»¶å®Œæ•´è·¯å¾„
    en_file = os.path.join(base_dir, f"{base_name}_EN{ext}")
    cn_file = os.path.join(base_dir, f"{base_name}_CN{ext}")

    # ç»Ÿä¸€æ¸…ç†è§„åˆ™ï¼šTrueä¿ç•™_CNåˆ _ENï¼ŒFalseä¿ç•™_ENåˆ _CN
    delete_file = en_file if use_en_setting else cn_file
    
    # æ”¶é›†å¾…æ¸…ç†æ–‡ä»¶ï¼ˆå»é‡ï¼‰
    if delete_file not in TO_CLEAN_SUFFIX_FILES and os.path.exists(delete_file):
        TO_CLEAN_SUFFIX_FILES.append(delete_file)

def batch_clean_extra_suffix_files() -> None:
    """
    æ‰¹é‡æ‰§è¡Œ_EN/_CNæ–‡ä»¶æ¸…ç†ï¼ˆæ‰€æœ‰æ›¿æ¢/é‡å‘½åå®Œæˆåç»Ÿä¸€æ‰§è¡Œï¼‰
    """
    if not TO_CLEAN_SUFFIX_FILES:
        print(f"â„¹ï¸ æ— éœ€è¦æ¸…ç†çš„_EN/_CNåç¼€æ–‡ä»¶")
        return
    
    print(f"\n=== å¼€å§‹æ‰¹é‡æ¸…ç†å¤šä½™çš„_EN/_CNåç¼€æ–‡ä»¶ ===")
    for delete_file in TO_CLEAN_SUFFIX_FILES:
        try:
            os.remove(delete_file)
            print(f"ğŸ—‘ï¸ å·²æ¸…ç†å¤šä½™åç¼€æ–‡ä»¶ï¼š{delete_file}")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†å¤šä½™åç¼€æ–‡ä»¶å¤±è´¥ {delete_file}ï¼š{e}")
    
    # æ¸…ç©ºå¾…æ¸…ç†åˆ—è¡¨
    TO_CLEAN_SUFFIX_FILES.clear()

# -------------------------- CSVæ–‡ä»¶äº¤æ¢é€»è¾‘ --------------------------
def swap_file_csv(file_path: str, file_name_without_extension: str, swap_fields: list) -> None:
    """
    å¤„ç†CSVæ–‡ä»¶äº¤æ¢ï¼ˆä»…å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œä¸ç«‹å³æ›¿æ¢åŸæ–‡ä»¶ï¼‰ï¼š
    1. ä¿ç•™å­—æ®µå†…çš„ä¸­æ–‡é€—å·
    2. ä»…äº¤æ¢æŒ‡å®šå­—æ®µçš„å€¼
    3. æ”¶é›†æ¸…ç†ä»»åŠ¡ï¼ˆä¸ç«‹å³æ¸…ç†ï¼‰
    """
    # åˆå§‹åŒ–æ•°æ®å­˜å‚¨
    dict_rows_now: List[Dict[str, str]] = []
    dict_rows_other: List[Dict[str, str]] = []

    # å¤„ç†è·¯å¾„
    abs_file_path = get_abs_file_path(file_path)
    script_dir = os.path.dirname(abs_file_path)
    file_ext = os.path.splitext(abs_file_path)[1]  # è·å–å¸¦ç‚¹çš„æ–‡ä»¶åç¼€ï¼ˆå¦‚ .csvï¼‰

    # æ„å»ºEN/CNæ–‡ä»¶è·¯å¾„
    en_file_name = f"{file_name_without_extension}_EN{file_ext}"
    cn_file_name = f"{file_name_without_extension}_CN{file_ext}"
    abs_path_en = os.path.join(script_dir, en_file_name)
    abs_path_cn = os.path.join(script_dir, cn_file_name)

    # éªŒè¯è¾“å…¥è·¯å¾„
    if file_name_without_extension in abs_file_path:
        abs_path_en = abs_file_path.replace(file_name_without_extension, f"{file_name_without_extension}_EN")
        abs_path_cn = abs_file_path.replace(file_name_without_extension, f"{file_name_without_extension}_CN")
    assert abs_file_path != abs_path_en, f"æ–‡ä»¶è·¯å¾„/åç§°è¾“å…¥é”™è¯¯ï¼š{abs_file_path} vs {abs_path_en}"

    # å†³å®šäº¤æ¢æ–¹å‘ï¼šä¼˜å…ˆä½¿ç”¨å…¨å±€è®¾ç½® USE_EN_SETTINGï¼ˆTrue è¡¨ç¤ºå½“å‰ä¸º ENï¼‰
    use_en = USE_EN_SETTING_NEW
    if use_en is None:
        raise Exception(f"aEP_UseEnStringè®¾ç½®è¯»å–å¤±è´¥")

    # è¯»å–ä¸»æ–‡ä»¶ï¼ˆä¸»æ–‡ä»¶å§‹ç»ˆå°è¯•è¯»å–ä¸ºåŸºç¡€æ•°æ®ï¼‰
    try:
        with open(abs_file_path, 'r', encoding='utf-8') as f:
            csv_reader = csv.DictReader(f)
            for row in csv_reader:
                row_id = row.get('id', '').strip()
                if not row_id or row_id.startswith('#'):
                    dict_rows_now.append(row)  # ä¿ç•™æ³¨é‡Šè¡Œï¼Œä¸å‚ä¸äº¤æ¢
                    continue
                dict_rows_now.append(row)
        print(f"ğŸ“„ æˆåŠŸåŠ è½½ä¸»æ–‡ä»¶ï¼š{abs_file_path}")
    except FileNotFoundError as e:
        raise Exception(f"ä¸»CSVæ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•è¯»å–ï¼š{abs_file_path}ï¼š{e}")
    except Exception as e:
        raise Exception(f"ä¸»CSVæ–‡ä»¶è¯»å–å¼‚å¸¸ï¼š{e}")

    # which path to read, is new language is EN, read _EN
    preferred = abs_path_en if USE_EN_SETTING_NEW else abs_path_cn

    # load file
    if preferred:
        try:
            with open(preferred, 'r', encoding='utf-8') as f:
                csv_reader = csv.DictReader(f)
                for row in csv_reader:
                    row_id = row.get('id', '').strip()
                    if not row_id or row_id.startswith('#'):
                        dict_rows_other.append(row)
                        continue
                    dict_rows_other.append(row)
            print(f"ğŸ“„ æˆåŠŸåŠ è½½è¯­è¨€æ–‡ä»¶ï¼š{preferred}")
        except FileNotFoundError as e:
            raise Exception(f"è¯­è¨€æ–‡ä»¶æœªæ‰¾åˆ°ï¼š{preferred}")
        except Exception as e:
            raise Exception(f"è¯­è¨€æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{e}ï¼š{preferred}")

    # æ£€æµ‹ç©ºæ•°æ®
    if not dict_rows_now:
        raise ValueError(f"ä¸»CSVæ–‡ä»¶è¯»å–åæ— æœ‰æ•ˆæ•°æ®ï¼š{abs_file_path}")
    if not dict_rows_other:
        raise ValueError(f"å¤‡ç”¨CSVæ–‡ä»¶ï¼ˆEN/CNï¼‰è¯»å–åæ— æœ‰æ•ˆæ•°æ®ï¼š{preferred if preferred else abs_path_en if os.path.exists(abs_path_en) else abs_path_cn}")

    # æå–æœ‰æ•ˆIDï¼ˆæ’é™¤æ³¨é‡Š/ç©ºIDï¼‰
    valid_ids_now = {
        row['id'].strip() for row in dict_rows_now
        if row.get('id', '').strip() and not row['id'].strip().startswith('#')
    }
    valid_ids_other = {
        row['id'].strip() for row in dict_rows_other
        if row.get('id', '').strip() and not row['id'].strip().startswith('#')
    }
    common_ids = valid_ids_now.intersection(valid_ids_other)
    print(f"ğŸ” æ‰¾åˆ°å¯äº¤æ¢çš„å…¬å…±IDæ•°é‡ï¼š{len(common_ids)}")

    # äº¤æ¢æŒ‡å®šå­—æ®µçš„å€¼
    for common_id in common_ids:
        # æ‰¾åˆ°å¯¹åº”IDçš„è¡Œ
        row_now = next((r for r in dict_rows_now if r['id'].strip() == common_id), None)
        row_other = next((r for r in dict_rows_other if r['id'].strip() == common_id), None)
        if not row_now or not row_other:
            continue

        # é€å­—æ®µäº¤æ¢
        for field in swap_fields:
            try:
                # è·³è¿‡ä¸å­˜åœ¨çš„å­—æ®µ
                if field not in row_now or field not in row_other:
                    print(f"âš ï¸ å­—æ®µ{field}åœ¨ID{common_id}ä¸­ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                    continue

                # ä¿ç•™åŸå§‹å€¼ï¼ˆå«ä¸­æ–‡é€—å·ï¼‰ï¼Œä»…äº¤æ¢å€¼
                val_now = row_now[field]
                val_other = row_other[field]

                # å¤„ç†å­—æ®µå†…çš„JSONæ ¼å¼å€¼ï¼ˆä¿ç•™ä¸­æ–‡é€—å·ï¼‰
                if val_now and val_other and val_now.startswith("{") and val_now.endswith("}") and val_other.startswith("{") and val_other.endswith("}"):
                    val_now = json.loads(clean_json_content(val_now))
                    val_other = json.loads(clean_json_content(val_other))

                # äº¤æ¢å€¼ï¼ˆä¿ç•™æ‰€æœ‰å­—ç¬¦ï¼ŒåŒ…æ‹¬ä¸­æ–‡é€—å·ï¼‰
                row_now[field] = str(val_other) if isinstance(val_other, (dict, list)) else val_other
                row_other[field] = str(val_now) if isinstance(val_now, (dict, list)) else val_now

            except Exception as e:
                raise Exception(f"ID{common_id}å­—æ®µ{field}äº¤æ¢å¤±è´¥ï¼š{e}")

    # å†™å…¥ä¸»æ–‡ä»¶åˆ°ä¸´æ—¶æ–‡ä»¶
    try:
        temp_main_path = write_to_temp_csv(dict_rows_now, abs_file_path)
        print(f"ğŸ“ ä¸»æ–‡ä»¶ä¸´æ—¶æ–‡ä»¶ç”Ÿæˆï¼š{temp_main_path}")
    except Exception as e:
        raise Exception(f"ä¸»æ–‡ä»¶ä¸´æ—¶æ–‡ä»¶å†™å…¥å¤±è´¥ï¼š{e}")

    # å†™å…¥å¤‡ä»½æ–‡ä»¶åˆ°ä¸´æ—¶æ–‡ä»¶
    try:
        # target_path: after swapping, write the swapped-out (original main) to the opposite suffix
        # We want the main file to be the "current" language (USE_EN_SETTING_NEW == true indicates EN).
        # Therefore the backup (swapped-out file) must be the other language suffix.      
        target_path = abs_path_cn if USE_EN_SETTING_NEW else abs_path_en
        temp_backup_path = write_to_temp_csv(dict_rows_other, target_path)
        print(f"ğŸ“ å¤‡ä»½æ–‡ä»¶ä¸´æ—¶æ–‡ä»¶ç”Ÿæˆï¼š{temp_backup_path}")
    except Exception as e:
        raise Exception(f"å¤‡ä»½æ–‡ä»¶ä¸´æ—¶æ–‡ä»¶å†™å…¥å¤±è´¥ï¼š{e}")

    # ========== ä¿®æ”¹ï¼šä»…æ”¶é›†æ¸…ç†ä»»åŠ¡ï¼Œä¸ç«‹å³æ‰§è¡Œ ==========
    collect_clean_task(script_dir, file_name_without_extension, file_ext, USE_EN_SETTING_NEW)

# -------------------------- JSONæ–‡ä»¶äº¤æ¢é€»è¾‘ --------------------------
def swap_json(file_path: str, file_name_without_extension: str, extension: str = None) -> None:
    """
    å¤„ç†JSON/factionæ–‡ä»¶äº¤æ¢ï¼ˆä»…å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œä¸ç«‹å³æ›¿æ¢åŸæ–‡ä»¶ï¼‰ï¼š
    1. ä¿ç•™å­—ç¬¦ä¸²å†…çš„ä¸­æ–‡é€—å·
    2. é€’å½’äº¤æ¢JSONå†…çš„å¯¹åº”å€¼
    3. æ”¶é›†æ¸…ç†ä»»åŠ¡ï¼ˆä¸ç«‹å³æ¸…ç†ï¼‰
    """
    # å¤„ç†è·¯å¾„
    abs_file_path = get_abs_file_path(file_path)
    script_dir = os.path.dirname(abs_file_path)
    # å¤„ç†æ‰©å±•åï¼šå¦‚æœä¼ å…¥extensionï¼ˆå¦‚factionï¼‰åˆ™ç”¨å®ƒï¼Œå¦åˆ™ä»è·¯å¾„æå–ï¼ˆä¸å¸¦ç‚¹ï¼‰
    file_ext_raw = extension if extension else os.path.splitext(abs_file_path)[1].lstrip('.')
    file_ext = f".{file_ext_raw}"  # è½¬ä¸ºå¸¦ç‚¹çš„æ‰©å±•åï¼ˆå¦‚ .jsonã€.factionï¼‰

    # æ„å»ºEN/CNæ–‡ä»¶è·¯å¾„
    en_file_name = f"{file_name_without_extension}_EN.{file_ext_raw}"
    cn_file_name = f"{file_name_without_extension}_CN.{file_ext_raw}"
    abs_path_en = os.path.join(script_dir, en_file_name)
    abs_path_cn = os.path.join(script_dir, cn_file_name)

    # ä¿®æ­£è·¯å¾„æ›¿æ¢é€»è¾‘
    if file_name_without_extension in abs_file_path:
        abs_path_en = abs_file_path.replace(file_name_without_extension, f"{file_name_without_extension}_EN")
        abs_path_cn = abs_file_path.replace(file_name_without_extension, f"{file_name_without_extension}_CN")

    # å†³å®šäº¤æ¢æ–¹å‘ï¼šä¼˜å…ˆä½¿ç”¨å…¨å±€è®¾ç½® USE_EN_SETTING
    use_en = USE_EN_SETTING_NEW
    if use_en is None:
        raise Exception(f"aEP_UseEnStringè®¾ç½®è¯»å–å¤±è´¥")

    data1 = None  # ä¸»æ–‡ä»¶æ•°æ®
    data2 = None  # è¯­è¨€æ–‡ä»¶æ•°æ®
    preferred = abs_path_en if USE_EN_SETTING_NEW else abs_path_cn

    # è¯»å–ä¸»æ–‡ä»¶
    try:
        data1 = read_json_safely(abs_file_path)
        print(f"ğŸ“„ æˆåŠŸåŠ è½½ä¸»æ–‡ä»¶ï¼š{abs_file_path}")
    except FileNotFoundError as e:
        raise Exception(f"ä¸»æ–‡ä»¶åŠ è½½å¤±è´¥ï¼š{e}")
    except Exception as e:
        raise Exception(f"JSONè¯»å–å¼‚å¸¸ï¼š{e}")

    # è¯»å–å¤‡ç”¨ï¼ˆä¼˜å…ˆä½¿ç”¨åå¥½ï¼‰
    if preferred:
        try:
            data2 = read_json_safely(preferred)
            print(f"ğŸ“„ æˆåŠŸåŠ è½½è¯­è¨€æ–‡ä»¶ï¼š{preferred}")
        except FileNotFoundError as e:
            raise Exception(f"è¯­è¨€æ–‡ä»¶æœªæ‰¾åˆ°ï¼š{preferred}")
        except Exception as e:
            raise Exception(f"è¯­è¨€æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{e}ï¼š{preferred}")

    # é€’å½’äº¤æ¢é€»è¾‘ä¿æŒä¸å˜
    def swap_nested_json_values(data1: Union[Dict, List], data2: Union[Dict, List]):
        if isinstance(data1, dict) and isinstance(data2, dict):
            common_keys = set(data1.keys()).intersection(data2.keys())
            for key in common_keys:
                if isinstance(data1[key], (dict, list)) and isinstance(data2[key], (dict, list)):
                    swap_nested_json_values(data1[key], data2[key])
                else:
                    temp = cp.deepcopy(data2[key])
                    data2[key] = cp.deepcopy(data1[key])
                    data1[key] = temp
        elif isinstance(data1, list) and isinstance(data2, list):
            min_len = min(len(data1), len(data2))
            for i in range(min_len):
                if isinstance(data1[i], (dict, list)) and isinstance(data2[i], (dict, list)):
                    swap_nested_json_values(data1[i], data2[i])
                else:
                    temp = cp.deepcopy(data2[i])
                    data2[i] = cp.deepcopy(data1[i])
                    data1[i] = temp

    swap_nested_json_values(data1, data2)

    # å†™å…¥ä¸»æ–‡ä»¶åˆ°ä¸´æ—¶æ–‡ä»¶
    try:
        temp_main_path = write_to_temp_json(data1, abs_file_path)
        print(f"ğŸ“ ä¸»æ–‡ä»¶ä¸´æ—¶æ–‡ä»¶ç”Ÿæˆï¼š{temp_main_path}")
    except Exception as e:
        raise Exception(f"ä¸»æ–‡ä»¶ä¸´æ—¶æ–‡ä»¶å†™å…¥å¤±è´¥ï¼š{e}")

    # å†™å…¥å¤‡ä»½ï¼ˆå†™åˆ°ä¸åå¥½ç›¸åçš„åç¼€ï¼‰
    try:
        target_path = abs_path_cn if USE_EN_SETTING_NEW else abs_path_en
        temp_backup_path = write_to_temp_json(data2, target_path)
        print(f"ğŸ“ å¤‡ä»½æ–‡ä»¶ä¸´æ—¶æ–‡ä»¶ç”Ÿæˆï¼š{temp_backup_path}")
    except Exception as e:
        raise Exception(f"å¤‡ä»½æ–‡ä»¶ä¸´æ—¶æ–‡ä»¶å†™å…¥å¤±è´¥ï¼š{e}")

    # ========== ä¿®æ”¹ï¼šä»…æ”¶é›†æ¸…ç†ä»»åŠ¡ï¼Œä¸ç«‹å³æ‰§è¡Œ ==========
    collect_clean_task(script_dir, file_name_without_extension, file_ext, USE_EN_SETTING_NEW)

# -------------------------- æ–‡ä»¶é‡å‘½åé€»è¾‘ --------------------------
def swap_name(file_path: str, file_name_with_ext: str) -> None:
    """é¢„æ”¶é›†é‡å‘½åä»»åŠ¡ï¼Œä¸ç«‹å³æ‰§è¡Œé‡å‘½å"""
    abs_file_path = get_abs_file_path(file_path)
    file_dir = os.path.dirname(abs_file_path)
    base_name, ext = os.path.splitext(file_name_with_ext)

    # æ„å»ºCN/ENæ–‡ä»¶å
    cn_file = f"{base_name}_CN{ext}"
    en_file = f"{base_name}_EN{ext}"
    cn_path = os.path.join(file_dir, cn_file)
    en_path = os.path.join(file_dir, en_file)

    # Always decide based on global USE_EN_SETTING (fallback to reading settings if None)
    use_en = USE_EN_SETTING_NEW
    if use_en is None:
        raise Exception(f"aEP_UseEnStringè®¾ç½®è¯»å–å¤±è´¥")

    # When USE_EN_SETTING_NEW is True, current language should be main(CN) + main_EN -> final files: main (EN) + main_CN
    # So if an EN file exists, move EN -> original and original -> _CN
    if USE_EN_SETTING_NEW:
        if os.path.exists(en_path):
            RENAME_TASKS.append((abs_file_path, cn_path, en_path))
            print(f"ğŸ“Œ é¢„æ”¶é›†é‡å‘½åä»»åŠ¡ï¼š{abs_file_path} -> {cn_path})")
        else:
            raise Exception(f"æœªæ‰¾åˆ°åç¼€ä¸ºXXX_ENçš„å¯¹åº”æ–‡ä»¶æ–‡ä»¶ï¼š{abs_file_path}")
    else:
        # USE_EN_SETTING_NEW is False, current language should be main(EN) + main_CN -> final files: main (CN) + main_EN
        # So if a CN file exists, move CN -> original and original -> _EN
        if os.path.exists(cn_path):
            RENAME_TASKS.append((abs_file_path, en_path, cn_path))
            print(f"ğŸ“Œ é¢„æ”¶é›†é‡å‘½åä»»åŠ¡ï¼šï¼š{abs_file_path} -> {en_path})")
        else:
            raise Exception(f"æœªæ‰¾åˆ°åç¼€ä¸ºXXX_CNçš„å¯¹åº”æ–‡ä»¶æ–‡ä»¶ï¼š{abs_file_path}")

def batch_execute_rename() -> None:
    """æ‰¹é‡æ‰§è¡Œé‡å‘½åä»»åŠ¡ï¼ˆç®€åŒ–ç‰ˆï¼Œç§»é™¤å†…éƒ¨æ¸…ç†é€»è¾‘ï¼‰
    æ ¸å¿ƒé€»è¾‘ï¼šåŸæ–‡ä»¶ â†” ç›®æ ‡åç¼€æ–‡ä»¶
    """
    for original_path, temp_path, swap_path in RENAME_TASKS:
        try:
            print(f"ğŸ” å¤„ç†é‡å‘½åä»»åŠ¡: {original_path}")

            # ========== æ ¸å¿ƒé‡å‘½åé€»è¾‘ ==========
            # 1. åŸæ–‡ä»¶ â†’ ä¸´æ—¶åç¼€æ–‡ä»¶ï¼ˆå¦‚ original.csv â†’ original_CN.csvï¼‰
            if os.path.exists(original_path):
                os.replace(original_path, temp_path)
                print(f"â¡ï¸ åŸæ–‡ä»¶å·²ç§»åŠ¨åˆ°ä¸´æ—¶ä½ç½®ï¼š{original_path} -> {temp_path}")
            else:
                print(f"âš ï¸ åŸæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ç§»åŠ¨ï¼š{original_path}")

            # 2. ç›®æ ‡äº¤æ¢æ–‡ä»¶ â†’ åŸæ–‡ä»¶ä½ç½®ï¼ˆå¦‚ original_EN.csv â†’ original.csvï¼‰
            if os.path.exists(swap_path):
                os.replace(swap_path, original_path)
                print(f"â¬…ï¸ äº¤æ¢æ–‡ä»¶å·²æ›¿æ¢åˆ°åŸä½ç½®ï¼š{swap_path} -> {original_path}")
            else:
                print(f"âš ï¸ äº¤æ¢æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ›¿æ¢ï¼š{swap_path}")

            # ========== ç§»é™¤ï¼šåŸæœ‰çš„æ¸…ç†é€»è¾‘ï¼Œæ”¹ä¸ºç»Ÿä¸€æ”¶é›†ä»»åŠ¡ ==========
            file_dir = os.path.dirname(original_path)
            base_name = os.path.splitext(os.path.basename(original_path))[0]
            ext = os.path.splitext(original_path)[1]
            collect_clean_task(file_dir, base_name, ext, USE_EN_SETTING_NEW)

        except Exception as e:
            print(f"âš ï¸ é‡å‘½åå¤±è´¥ {original_path}ï¼š{e}")
            raise  # æŠ›å‡ºå¼‚å¸¸ç»ˆæ­¢æ‰§è¡Œ

    # æ¸…ç©ºé‡å‘½åä»»åŠ¡åˆ—è¡¨
    RENAME_TASKS.clear()

# -------------------------- JSONé…ç½®æ›´æ–°é€»è¾‘ --------------------------
def update_setting_in_json(file_path: str, key: str, new_value: Any = None) -> Any:
    """æ›´æ–°JSONé…ç½®ï¼ˆä¿ç•™ä¸­æ–‡é€—å·ï¼Œå†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼‰

    å¦‚æœ new_value ä¸º Noneï¼Œåˆ™è¯»å–å½“å‰å€¼å¹¶å¯¹å¸ƒå°”å€¼å–ååå†™å›ï¼ˆè¿”å›æ–°çš„å€¼ï¼‰ã€‚
    è¿”å›å†™å…¥åçš„å€¼ï¼Œä¾›è°ƒç”¨è€…ä½¿ç”¨ã€‚
    """
    abs_file_path = get_abs_file_path(file_path)
    try:
        # ç›´æ¥è¯»å–åŸå§‹å†…å®¹å¹¶æ¸…ç†ï¼Œç¡®ä¿æˆ‘ä»¬è§£æåˆ°å®é™…çš„å­—å…¸ç»“æ„
        with open(abs_file_path, 'r', encoding='utf-8-sig') as f:
            raw = f.read()
        if raw is None or raw.strip() == '':
            raise ValueError(f"é…ç½®æ–‡ä»¶å†…å®¹ä¸ºç©ºï¼š{abs_file_path}")
        clean = clean_json_content(raw)
        data = json.loads(clean)

        if not isinstance(data, dict):
            raise ValueError(f"é…ç½®æ–‡ä»¶è§£æåä¸æ˜¯å­—å…¸ï¼š{abs_file_path}")

        # ä»…ä¿®æ”¹æŒ‡å®šé”®ï¼ˆä¿ç•™å…¶å®ƒé”®ï¼‰
        if new_value is None:
            if key not in data:
                raise KeyError(f"é”®{key}ä¸å­˜åœ¨äºé…ç½®æ–‡ä»¶ä¸­ï¼ˆå¯ç”¨é”®ï¼š{list(data.keys())}ï¼‰")
            if not isinstance(data[key], bool):
                raise ValueError(f"é”®{key}ä¸æ˜¯å¸ƒå°”å€¼ï¼Œæ— æ³•å–åï¼ˆå½“å‰å€¼ï¼š{data[key]}ï¼Œç±»å‹ï¼š{type(data[key])}ï¼‰")
            data[key] = not data[key]
        else:
            data[key] = new_value

        print(f"ğŸ”§ æ›´æ–°é…ç½®ï¼š{key} = {data[key]}")

        # å°†ä¿®æ”¹åçš„å®Œæ•´å­—å…¸å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆä¸ä¼šä¸¢å¤±å…¶å®ƒé”®ï¼‰
        json_content = json.dumps(data, ensure_ascii=False, indent=2)
        temp_config_path = create_temp_file(json_content, abs_file_path)
        print(f"ğŸ“ é…ç½®æ–‡ä»¶ä¸´æ—¶æ–‡ä»¶ç”Ÿæˆï¼š{temp_config_path}")

        return data[key]
    except FileNotFoundError:
        raise Exception(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{abs_file_path}")
    except json.JSONDecodeError as e:
        raise Exception(f"é…ç½®æ–‡ä»¶JSONè§£æå¤±è´¥ï¼š{abs_file_path}ï¼š{e}")
    except Exception as e:
        raise Exception(f"é…ç½®æ›´æ–°å¼‚å¸¸ï¼š{e}")

# -------------------------- ä¸»æ‰§è¡Œé€»è¾‘ï¼ˆåŸå­æ€§æ‰¹é‡å¤„ç†ï¼‰ --------------------------
if __name__ == "__main__":
    try:
        # åˆå§‹åŒ–å…¨å±€è¯­è¨€è®¾ç½®ï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
        USE_EN_SETTING_OLD = get_current_aep_setting()
        USE_EN_SETTING_NEW = not USE_EN_SETTING_OLD

        # ========== ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼Œç”Ÿæˆä¸´æ—¶æ–‡ä»¶/æ”¶é›†é‡å‘½åä»»åŠ¡ ==========
        print("=== å¼€å§‹å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼Œç”Ÿæˆä¸´æ—¶æ–‡ä»¶ ===")

        # CSVæ–‡ä»¶äº¤æ¢ï¼ˆç”Ÿæˆä¸´æ—¶æ–‡ä»¶ + æ”¶é›†æ¸…ç†ä»»åŠ¡ï¼‰
        swap_file_csv("data/campaign/submarkets.csv", "submarkets", ['name', 'desc'])
        swap_file_csv("data/campaign/rules.csv", "rules", ['script','text','options'])
        swap_file_csv("data/campaign/industries.csv", "industries", ['name','desc'])
        swap_file_csv("data/campaign/special_items.csv", "special_items", ['name','tech/manufacturer','desc'])
        swap_file_csv("data/campaign/commodities.csv", "commodities", ['name'])
        swap_file_csv("data/campaign/market_conditions.csv", "market_conditions", ['name','desc'])
        swap_file_csv("data/strings/descriptions.csv", "descriptions", ['text1','text2','text3','text4','text5'])
        swap_file_csv("data/characters/skills/skill_data.csv", "skill_data", ['name','description','author'])
        swap_file_csv("data/shipsystems/ship_systems.csv", "ship_systems", ['name'])
        swap_file_csv("data/hulls/ship_data.csv", "ship_data", ['name','tech/manufacturer','designation'])
        swap_file_csv("data/hullmods/hull_mods.csv","hull_mods",['name','tech/manufacturer','uiTags','desc','short','sModDesc'])
        swap_file_csv("data/weapons/weapon_data.csv","weapon_data",['name','tech/manufacturer','primaryRoleStr','customPrimary','customAncillary'])
        swap_file_csv("data/config/LunaSettings.csv", "LunaSettings", ['fieldName','fieldDescription' ])

        # æ–‡ä»¶é‡å‘½åï¼ˆé¢„æ”¶é›†ä»»åŠ¡ï¼‰
        swap_name("data/missions/aEP_eliminate_mission/descriptor.json", "descriptor.json")
        swap_name("data/missions/aEP_eliminate_mission/mission_text.txt", "mission_text.txt")
        swap_name("data/missions/aEP_first_contact/descriptor.json", "descriptor.json")
        swap_name("data/missions/aEP_first_contact/mission_text.txt", "mission_text.txt")
        swap_name("data/missions/aEP_planet_investigation/descriptor.json", "descriptor.json")
        swap_name("data/missions/aEP_planet_investigation/mission_text.txt", "mission_text.txt")
        swap_name("data/missions/aEP_assassination/descriptor.json", "descriptor.json")
        swap_name("data/missions/aEP_assassination/mission_text.txt", "mission_text.txt")
        swap_name("data/missions/aEP_test/descriptor.json", "descriptor.json")
        swap_name("data/missions/aEP_test/mission_text.txt", "mission_text.txt")
        swap_name("data/missions/aEP_random_fleet_combat/descriptor.json", "descriptor.json")
        swap_name("data/missions/aEP_random_fleet_combat/mission_text.txt", "mission_text.txt")

        # JSON/factionæ–‡ä»¶äº¤æ¢ï¼ˆç”Ÿæˆä¸´æ—¶æ–‡ä»¶ + æ”¶é›†æ¸…ç†ä»»åŠ¡ï¼‰
        swap_json("mod_info.json","mod_info")
        swap_json("data/config/modFiles/magicBounty_data.json", "magicBounty_data")
        swap_json("data/config/planets.json", "planets")
        swap_json("data/config/custom_entities.json", "custom_entities")
        swap_json("data/world/factions/aEP_FSF.faction", "aEP_FSF","faction")
        swap_json("data/world/factions/aEP_FSF_adv.faction", "aEP_FSF_adv","faction")

        # é…ç½®æ›´æ–°ï¼ˆç”Ÿæˆä¸´æ—¶æ–‡ä»¶ï¼‰
        # update_setting_in_json("data/config/settings.json", 'aEP_UseEnString', None)

        # ========== ç¬¬äºŒæ­¥ï¼šæ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼Œæ‰¹é‡æ‰§è¡Œæ›¿æ¢/é‡å‘½å ==========
        print("\n=== æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼Œå¼€å§‹æ‰¹é‡æ›¿æ¢åŸæ–‡ä»¶ ===")
        # æ‰¹é‡æ›¿æ¢ä¸´æ—¶æ–‡ä»¶ä¸ºåŸæ–‡ä»¶
        batch_replace_original_files()
        # æ‰¹é‡æ‰§è¡Œé‡å‘½åä»»åŠ¡
        batch_execute_rename()

        # æœ€åæ›´æ–°è®¾ç½®ï¼ˆåœ¨é‡å‘½ååæ‰§è¡Œï¼Œä»¥ä¾¿åŸºäºæœ€æ–°æ–‡ä»¶åç¼€çŠ¶æ€ï¼‰
        update_setting_in_json("data/config/settings.json", 'aEP_UseEnString', None)
        # å°†è®¾ç½®ä¸´æ—¶æ–‡ä»¶åº”ç”¨åˆ°ç£ç›˜
        batch_replace_original_files()

        # ========== ç¬¬ä¸‰æ­¥ï¼šæ‰€æœ‰æ›¿æ¢/é‡å‘½åå®Œæˆåï¼Œç»Ÿä¸€æ¸…ç†_EN/_CNæ–‡ä»¶ ==========
        batch_clean_extra_suffix_files()

        print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶äº¤æ¢/é‡å‘½å/æ¸…ç†å®Œæˆï¼")

    except Exception as e:
        # ä»»æ„æ­¥éª¤å¤±è´¥ï¼Œæ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ï¼Œç»ˆæ­¢æ“ä½œ
        print(f"\nâŒ å¤„ç†å¤±è´¥ï¼š{e}")
        print("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        clean_temp_files()
        exit(1)

    # æœ€åæ¸…ç†ç©ºçš„ä¸´æ—¶æ–‡ä»¶åˆ—è¡¨ï¼ˆå†—ä½™ä¿æŠ¤ï¼‰
    clean_temp_files()